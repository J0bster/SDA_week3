{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDA: Assignment 3\n",
    "\n",
    "**Name**:\n",
    "\n",
    "**Student ID**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous two assignments you have focused on cross-sectional data where the values in each data sample are i.i.d. and have no particular ordering. In this assignment we will study time-series where the values in each data sample have a specific ordering. We will start in Problem 1 still in a cross-sectional but paired samples setting, which in the subsequent problems is applied to studying real time-series.\n",
    "In this assignment you will learn:\n",
    "1. The concept of statistical dependence between two variables;\n",
    "2.  Linear Regression Model;\n",
    "3.  A general fitting procedure for model parameters;\n",
    "4. The concept of correlation and of variance explained; \n",
    "5. Generating your own time-series;\n",
    "6. The concept of order of integration and co-integration; \n",
    "7. The pitfall of and solution to spurious correlation.\n",
    "\n",
    "## Coding and References\n",
    "You should use the Python programming language for all programming tasks. You should use native Python code for all tasks unless otherwise instructed. Whenever you have already implemented a particular function before by yourself, from that point onward you are allowed to use the more efficient implementation from a package. So for instance, if you have already implemented yourself the `mean()` function and have used it at least once in an answer then you are allowed to use the `np.mean()` function from numpy.\n",
    "\n",
    "The exceptions are:\n",
    "1. the logarithm function, which you may find in the packages `math` or `numpy`.\n",
    "2. the square root function √x, which can be computed using `x**0.5`.\n",
    "Further Exception (only week 4): the use of vectors and matrices (`numpy.array`) is additionally allowed in week 4's assignment. (Essentially I assume that you know how to implement multidimensional arrays by nesting lists, so no need to show that explicitly.) Still, before you use the `np.dot` (dot product) function from numpy, for instance, implement it yourself first to show that you know what it does exactly. Also for clustering, such as k-means clustering: implement it yourself first and use it at least once to show that it works. If needed for performance reasons you can subsequently use the implementation from the `sklearn` package, for example.  \n",
    "\n",
    "For some questions you may have to look up certain facts, formulas, equations, or freshen up your knowledge. The problems in the assignments do not always refer to pertinent sources; you are expected to research into missing knowledge yourself. \n",
    "For refreshers of the very basics of probability theory, or additional resources behind statistical concepts introduced here, please look on Canvas for the resources provided.\n",
    "\n",
    "Please do not ever copy (parts of) your answer or code from any other source, be it online or from fellow students. You can help each other understand the problem and to compare outcomes, but not copy answers.\n",
    "\n",
    "## Submission\n",
    "Please upload your answers to all questions in Jupyter Notebook format (.ipynb extension) through the appropriate Canvas page no later than Sunday, November 17th, 2024 at 23:59. Your answers and derivations should be included inline in your Jupyter Notebook file (do not remove any output before saving the file). Be sure to make every step explicit and clearly identifiable (computational performance or code brevity are not graded). Submissions by e-mail or past this deadline will not be considered.\n",
    "Deadline extension requests will not be considered unless they are mediated through a study advisor.\n",
    "\n",
    "## Grading\n",
    "You can earn a maximum of 112 points. In case you do not submit both your answers and code by the deadline then you receive an “NA”. If you submit your answers and code but score 0 points then your grade will be a 1 (Dutch grading system). Scoring all 112 points means your grade becomes a 10. An amount of bonus points can be earned which is added to your score to a maximum of 112. That is, each week you can earn a maximum grade of 10 and it is not possible to transfer (bonus) points from one assignment to another.\n",
    "\n",
    "Each of the weekly assignments will have a relative weight proportional to the maximum number of points. In total they make up 50% of your final grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:52:03.280400Z",
     "start_time": "2021-11-17T10:52:03.261785Z"
    }
   },
   "source": [
    "**LINK TO PRELIMINARY MATERIAL FOR THE COURSE** https://www.overleaf.com/read/tygcjfmjjpss\n",
    " Here you can find a pdf, which is continuous update, to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR OTHER IMPORTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In this problem you start in a cross-sectional but paired samples setting, which in the subsequent problems\n",
    "is applied to studying real time-series. Remember, capital letters stand for stochastic variables, lower-case\n",
    "letters stand for a specific value, and a vector like $\\vec{s}$ denotes a data sample containing multiple values (whose\n",
    "ordering will start to matter in this assignment as we deal with time series).\n",
    "\n",
    "Let us consider a data sample of paired observations $\\overrightarrow{S}_n = (X_1, Y_1), \\dots, (X_n, Y_n)$. For $Y_i$ let us consider the statistical model\n",
    "\n",
    "\\begin{align*}\n",
    "    Y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2_{Y\\mid X})\\quad\\quad\\quad\\quad\\text{(1)}\n",
    "\\end{align*}\n",
    "\n",
    "That is, each $Y_i$ follows a normal distribution with exactly the same variance $\\sigma^2_{Y\\mid X}$ but with a different mean.\n",
    "The reason for the notation of $\\sigma^2_{Y\\mid X}$ will become clear during the problems. During this entire assignment we\n",
    "will only deal with the *linear regression* model which is the simplest possible model to capture dependencies\n",
    "between paired values, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu_i = \\alpha + \\beta \\cdot X_i\\quad\\quad\\quad\\quad\\quad\\text{(2)}\n",
    "\\end{align*}\n",
    "\n",
    "In this model, $\\alpha$ is also called the *intercept* and $\\beta$ is also called the *slope*.\n",
    "For completeness, since Eq. 1 and Eq. 2 define the conditional probability $Pr (Y_i \\mid X_i )$, all we need in order\n",
    "to have a complete statistical description $Pr(X_i , Y_i )$ of each sample is the marginal probability distribution\n",
    "$Pr(X_i )$. A typical distribution in the cross-sectional case is a normal distribution. A typical and simple distribution in the time-series case is $X_i \\sim \\mathcal{N}(i, \\sigma^2_X)$, where $\\sigma_X$ is small or even $\\sigma_X \\rightarrow 0$ i.e., $X_1,\\dots,X_n$ denotes a regular interval (time stamps?) with possibly some variation. \n",
    "\n",
    "In this first problem we will consider the cross-sectional case with a normal distribution, so $X_i \\sim \\mathcal{N}(\\mu_X, \\sigma^2_X)$ (so the mean does not depend on time yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "Write a function `randpair(mu_x, sigma_x, alpha, beta, sigma_y_x)` which will generate a\n",
    "random pair $(x_i , y_i )$ and returns it as a tuple of length 2. For computational efficiency you may also\n",
    "use the `np.random.normal` instead of your own implementation.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randpair(mu_x, sigma_x, alpha, beta, sigma_y_x):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 \n",
    "Generate a data sample of $n = 10^3$ paired values which we will name $\\vec s$ . As parameter values use:\n",
    "$\\mu_X = 0, \\sigma_X = 1, \\alpha = 0, \\beta = 4.5, \\sigma_{Y\\mid X}  = 1.50$.\n",
    "\n",
    "Show a scatter plot of this data sample such that\n",
    "the $x_i$ value is on the x-axis and the $y_i$ value is on the y-axis. For evident reasons such plots are also\n",
    "colloquially referred to as ‘cigar plots’. Overlay the corresponding line which describes the theoretically\n",
    "expected value of $Y$ as function of $X$.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "Now let us pretend that we received this data sample $\\vec s$ and do not know its generating model. Just as in the previous assignments there will be a *maximum likelihood estimate* for the intercept $\\hat \\alpha$, the slope\n",
    "$\\hat \\beta$, and the noise level $\\hat \\sigma_{Y \\mid X}$, but also a confidence interval for each of them. This will be important in the subsequent problems.\n",
    "\n",
    "Your task will be to find the three parameters which maximize the conditional probability of the data sample to $\\text{Pr}(\\vec S\\mid \\hat \\alpha, \\hat \\beta, \\hat \\sigma_{Y \\mid X}) = \\text{Pr}(\\vec X, \\vec Y\\mid \\hat \\alpha, \\hat \\beta, \\hat \\sigma_{Y \\mid X})$, which will be spit over multiple questions.\n",
    "\n",
    "It is important to realize that the choice of the linear regression model (intercept, slope, noise level) has nothing to do with the marginal probabilities of the $x_i$ values, so $\\text{Pr}(X_i\\mid  \\alpha,  \\beta,  \\sigma_{Y \\mid X}) = P(X_i)$. Therefore, to start, use standard probability theory<sup>1</sup> to derive step by step the fact that the *optimization problem* reduces to maximizing the log-likelihood $\\log L( \\vec Y\\mid \\vec X,\\hat \\alpha, \\hat \\beta, \\hat \\sigma_{Y \\mid X})$, where the probabilities of the $x_i$ no longer appear on the left of the vertical bar.\n",
    "\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 1: In particular, use the generalization of the rule $Pr(A, B) = Pr(A | B) Pr(B)$ which is: $Pr(A, B | C) = Pr(A | B, C) Pr(B | C)$.\n",
    "The symbols $A, B, C$ may either constitute a single stochastic variable or a combination (intersection) of stochastic variables.</span>\n",
    "\n",
    "[6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "There is a crucial assumption that you will need to make regarding this cross-sectional sample $\\vec s$ , which\n",
    "was also made in Assignment 1. Name this assumption and write the new formula for the log-likelihood\n",
    "that this assumption enables, which is needed for being able to calculate the log-likelihood.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "Now you are ready to implement `loglikelihood_lrm(alpha, beta, sigma_y, sample)` which\n",
    "will compute the log-likelihood of the given list of paired values (i.e., `sample`) given the linear regression model defined by the parameters $\\alpha, \\beta, \\sigma_{Y\\mid X}$ . For inspiration you may want to look back at your Assignment 1.\n",
    "\n",
    "For computational speed, the probability density of value $x$ given a normal distribution $\\mathcal{N}(\\mu, \\sigma)$\n",
    "may also be calculated using `st.norm.pdf(x, loc=mu, scale=sigma)`. You may test your implementation using your answer for question 2; the\n",
    "correct parameter values should yield a higher likelihood than other parameter values (such a setting\n",
    "the slope to zero).\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_lrm(alpha, beta, sigma_y, sample):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6\n",
    "\n",
    "Although there is a simple analytical way to estimate the parameters for the linear regression, in general\n",
    "there is not, so you will now implement a general procedure. Implement a function `fit_lrm(sample)` which takes a list of 2-tuples as input and returns a tuple $(\\hat \\alpha, \\hat \\beta, \\hat \\sigma_{Y \\mid X})$ with the estimated MLE values\n",
    "for the intercept, slope, and noise level of a linear regression model (LRM). Finding the parameter\n",
    "values which maximize a function is called an optimization procedure. You are free regarding the\n",
    "optimization technique you use, as long as it is implemented by yourself. Simple possibilities include:\n",
    "\n",
    "* Generating many random parameter values within certain ranges and remembering the parameter set which maximizes the log-likelihood;\n",
    "* A nested for-loop, one for each parameter, with a constant step size for each parameter value within a certain range, so that a regular grid of parameter values is considered;\n",
    "* (Bonus +2) A nested version of one of the previous options: first a large range of values is considered, and then in each subsequent iteration a smaller range of parameter values is considered around the best solution of the previous iteration (2 or 3 iterations may be practically sufficient)\n",
    "* (Bonus +2) Greedy gradient ascent: select a random parameter set, compute the log-likelihood, then generate a random vector with small norm (small individual values) which is added to this parameter set, compute the log-likelihood of this new parameter set: if it is higher, take the new parameter set as your ‘current’ set, otherwise keep your current set as it is. Stop the search when a sufficient number of times your parameter set did not improve.\n",
    "\n",
    "Again, you may want to test your implementation using the sample you have already generated and\n",
    "the corresponding known parameter values which this function should reproduce approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning :** If you use a package implementation of the gaussian pdf, beware of the same problem you have already encountered in assignment 1 when computing the log of a too low value for the pdf. \n",
    "\n",
    "$log (e^{-100000})$ should be equal to -100000 but your machine will approximate $e^{-100000}$ as $0$ and so you may want to implement the simplification with the logarithm directly in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'Cutting Corners'.** To narrow your search range and thus speed up the optimization procedure you\n",
    "may adjust the ranges in which you search. For instance, depending on how the scatter plot looks you may decide that the slope can never be negative. Excluding such options may greatly reduce the time it\n",
    "takes for your optimization to complete<sup>2</sup>. You may also extend the argument list of your optimization\n",
    "function with the ranges in which the search should take place, if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 2: The second option takes about one minute to complete on my laptop for the case of 10 × 10 × 10 parameter sets. So\n",
    "100 × 100 × 100, which is better, would take about 1000 minutes or about 16 hours.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Failsafe.** In case you do not (yet) manage to implement your own optimization procedure you may use the following implementation of the function. Naturally, using this implementation will not earn you any points for this question; in all subsequent questions you can then still earn full points. Nevertheless, in subsequent questions and problems you will be allowed to use this provided implementation instead of your own implementation for the sake of accuracy and speed, unless explicitly stated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "\n",
    "def failsafe_fit_lrm(sample):\n",
    "    optres = minimize(lambda tup: -loglikelihood_lrm(*tup, sample=sample),\n",
    "                     (0, 0, 1),\n",
    "                     bounds=((None, None), (None, None), (0.001, None)))\n",
    "    return optres.x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6 points] \n",
    "\n",
    "[2 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lrm(sample):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7\n",
    "Execute your own implementation of fit lrm(sample) on the data sample $\\vec s$ and show that it approximates (with high probability, depending on your implementation) the true parameter values to within at most 0.3 absolute difference.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8\n",
    "\n",
    "Great, you can now estimate the MLE parameter values for (in this case) a linear regression. Let us now turn to the CI of each parameter value, i.e., the uncertainty due to the fact that we estimate this from a finite sample\n",
    "\n",
    "Let us first do this parametrically and using the LRM parameters you fitted under question 7. For $X$ simply estimate directly the two required parameter values from the dataset you generated in question 1.2. For $Y$ use your fitting procedure. In total you should now have 5 parameter values. \n",
    "Use these parameter values to generate $N = 10^3$ datasets, each consisting of $n = 10^3$ samples. Estimate the three LRM parameter values for each sample\n",
    "using an optimization procedure. Show the three histograms for the LRM parameter values. Then\n",
    "estimate the 95% CI for each parameter using a percentile function and report or plot them. \n",
    "Use the CI for the parameter $\\beta$ to test the hypothesis $H_{0}$:\n",
    "\n",
    "$H_{0}:  𝛽=0$, \n",
    "\n",
    "The alternative hypothesis in this case is : $H_{1}:  𝛽\\neq0$.\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note: this calculation will take quite a long time, so you may want to use some smaller values for N and n for the development and testing and then run for the requested values later or overnight.</span>\n",
    "\n",
    "[7 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9\n",
    "Now repeat this hypothesis test but for the case of only n = 4. Explain briefly the difference between\n",
    "these two hypothesis tests and why it makes intuitive sense.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10\n",
    "\n",
    "Implement a non-parametric bootstrap procedure to estimate the expected value and CI of each parameter as well, in a separate function such as bootstrap lrm(sample). Use a sufficient number of resamples. Show that the CIs of this method correspond fairly well with those of question 8 and\n",
    "question 9.\n",
    "\n",
    "[6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11\n",
    "\n",
    "The question whether the slope could be zero was not an arbitrary one (question 8 and question 9). A non-zero slope indicates a statistical relationship between the two variables. This means that if the\n",
    "slope is non-zero then knowing a value of $x_i$ means that we can better predict the corresponding value of $y_i$ compared to the case where we do not know $x_i$ . Explain briefly in your own words why a slope of zero means that there cannot be any improved prediction from knowing $x_i$.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12\n",
    "\n",
    "An LRM is very closely related to the classical (linear) correlation coefficient, which we will get to in a moment. The most well-known measure to quantify the goodness-of-fit of a model (in our case, an LRM) is the percent of variance ‘explained’ or percent of variance reduction.<sup>3</sup>\n",
    "\n",
    "Using $\\vec s$ , first compute the variance of $Y_i$ and then compute the variance of the so-called residuals\n",
    "$Y_i − (\\alpha + \\beta \\cdot X_i )$ of the LRM you fitted to this dataset. Compute the percent of variance reduction\n",
    "obtained by knowing the value of $X_i$ versus not knowing it. This quantity should be higher than 50%. Report the two variances and the percent reduction.\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 3: I like the explanation of http://www.sportsci.org/resource/stats/linreg.html and further pages, such as the\n",
    "‘goodness-of-fit’ page.</span>\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13\n",
    "\n",
    "Suppose that for some reason we use a very wrong LRM model by taking the negative value for $\\hat \\beta$. Using ‘wrong’ models happens more often than you think, for instance by fitting a model to one phenomenon (weather in USA) and then trying to apply it to a very different phenomenon (weather in Europe). Or creating a model for the innate immune response in human tissue in the first 36 hours and then trying to predict something after 48 hours or more.\n",
    "\n",
    "For this ‘wrong’ model again compute the goodness-of-fit. Explain in words why the obtained value makes sense.\n",
    "\n",
    "[5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14\n",
    "Explain briefly why an increasing value for the slope β for the generating model does not imply an\n",
    "increasing goodness-of-fit. Which parameter of the LRM is more directly controlling the goodness-of-\n",
    "fit?\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15\n",
    "\n",
    "The most well-known and classical correlation coefficient for cross-sectional samples is the Pearson\n",
    "product-moment coefficient $−1 ≤ r ≤ 1$.<sup>4</sup> It is strongly related to the LRM.\n",
    "Calculate $r$ for your dataset $\\vec s$ . It is common knowledge that $r^2$ produces the goodness-of-fit of this\n",
    "correlation function (which we will not derive mathematically here). Compare $r^2$ with your answer to question 12.\n",
    "Still, we usually report $r$ in statistical studies and not r^2 . Name one clear advantage of this.\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 4: See,\n",
    "e.g., https://www.wikiwand.com/en/Correlation_and_dependence.</span>\n",
    "\n",
    "[6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.16\n",
    "\n",
    "Three questions. What should be the value of $r^2$ (i.e., percent of variance reduction) if it is true that $σ_{Y|X} = {σ_Y}$ , where $σ_Y$ is the standard deviation of $Y$ , and $β = 0$? What should be the value of $r^2$\n",
    "when $σ_{Y|X} = 0$ and $β \\neq 0$ (and then regenerate the dataset)? What would happen to $r^2$ if you would increase $σ_{Y |X}$ while keeping the $β \\neq 0$ constant?\n",
    "\n",
    "[6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "In the previous problem you still considered cross-sectional samples of i.i.d. paired values. In this problem\n",
    "you will transition to time-series. A time-series is a sequence of observations where each observation depends\n",
    "on (has a statistical relation with) the previous observation(s). This is a transition that we are doing carefully\n",
    "because there are some caveats. Cross-sectional techniques should not be blindly applied to time-series, as\n",
    "you will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "\n",
    "It is very easy to generate your own time-series. First generate a cross-sectional sample $\\vec d$ consisting of\n",
    "$n = 10^2$ i.i.d. values drawn from $\\mathcal N (μ = 3, σ = 6)$. Then create the time-series $\\vec s_t$ by computing the running sum, i.e., $\\vec s_t = \\sum_{i=0}^t \\vec d_i$ . Show the plot of $\\vec s_t$ versus $t$. As an aside, this time-series is now called a *[first-order integrated](https://www.wikiwand.com/en/Order_of_integration)* time-series because it consists of one integration step of a so-called *stationary process* (in our case: i.i.d. values).\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "The previous problem gives insights into analyzing this time-series. To see this, plot the scatter plot of $n − 1$ points $(\\vec s_{t−1} , \\vec s_t)$.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Fit an LRM to this time-series using your implemented function. Use $x_t = t$ to make the list of\n",
    "paired values that your optimization function expects. Why is it incorrect to fit statistical parameters\n",
    "describing $X_i$ in this case, in contrast to the previous problem?\n",
    "\n",
    "Report the three fitted parameter values of the LRM. Which values for the intercept and slope should\n",
    "theoretically be expected?\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "You may already have noticed one deviation from the cross-sectional case here: the fitted $σ_{Y|X}$ parameter. It tends to be significantly and structurally larger than the standard deviation of the i.i.d. sample\n",
    "we started with (which is $σ$). In fact, we should ignore this fitted parameter value; more accuracutely,\n",
    "this fitted parameter does not estimate $\\sigma$. This can be seen, e.g., when drawing the LRM line on\n",
    "top of the time-series and looking closely at the behavior of the deviations (residuals) from this line.\n",
    "Explain briefly which assumption is violated in the optimization procedure to estimate $σ$.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "\n",
    "Briefly describe a procedure to correctly estimate $σ$. Estimate its numerical value in this way. \n",
    "\n",
    "**Fail-safe:**\n",
    "simply use the theoretical value used to generate the time-series.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "\n",
    "Using your fitted LRM to generate a single prediction curve of how $\\vec s_t$  could continue for the next $n$\n",
    "time steps. Attach this time-series prediction to $\\vec s_t$ and plot it in a different color.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 \n",
    "\n",
    "Of course this is just one prediction out of very many that you could have made with this LRM. Often we want to know the range of predictions that could be made. Therefore, compute 1000 such predictions using your fitted LRM and plot them all using the same transparent color<sup>7</sup> .\n",
    "\n",
    "Of these 1000 predictions, compute the mean and the CI per time point. Show these three curves in opaque color on top of the same plot. Explain briefly why the CI range widens and why that should\n",
    "be expected.\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 5: Transparency (90%), a color (blue), and a linewidth (1) can be set using `plt.plot(x, y, ’-’, alpha=0.1, color=(0.0,0.0,1.0), linewidth=1)`.</span>\n",
    "\n",
    "[6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8\n",
    "\n",
    "The stochasticity of a single prediction model (here LRM) is not the only source of prediction uncertainty. Estimate the mean and standard deviation (so not CI) of each parameter of the LRM (see also\n",
    "problem 8 in previous question for inspiration) and report the numerical values. Let us assume that\n",
    "each parameter’s distribution is a normal distribution.\n",
    "\n",
    "Generate 25 parameter estimates for each parameter. Compute 1000 predictions for each of the 25 parameter sets. Overlay the 25 prediction uncertainties (i.e., the 1000 curves) in optionally different colors\n",
    "per parameter set onto the same plot. Estimate and show again the mean and CI curves but now of\n",
    "all the 25000 predictions taken together. Explain the difference of the CI with that of the previous\n",
    "question.\n",
    "\n",
    "[5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T13:33:27.125893Z",
     "start_time": "2021-11-12T13:33:27.116626Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "\n",
    "Actually, there is yet another source of uncertainty. You have so far fitted an LRM using the entire time-series $\\vec s_t$. Sometimes this is not appropriate. Think of weather predictions for example. The temperature of the past few days is more pertinent to tomorrow’s temperature than that of a few\n",
    "months ago. At a different level, think of the average temperature over a year. This should be a smooth cyclic behavior through the seasons. On the scale of a month or two we can use approximately\n",
    "an LRM to roughly estimate next month’s temperature, but fitting an entire year should result in slope\n",
    "0 (unless we’re headed towards the next snowball earth phase). Therefore it may pay off to study the\n",
    "short-term trends, meaning that only the last $n'$ points are used for regression instead of all $n$.\n",
    "\n",
    "Describe briefly in words how you could study the added uncertainty from the choice of $n'$.\n",
    "\n",
    "[3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**Bonus problem.** A common task is to find correlations between time-series, such as between global\n",
    "temperature and local sea level, or between two financial indices. But a very common mistake is made due\n",
    "to spurious regression, including by myself, which therefore I feel obligated to teach you the idea behind it.\n",
    "There is a lot more to say about it than handled here, but I hope the basic idea comes across.\n",
    "The following are some very basic (and perhaps too basic) intuitive descriptions of important and relevant\n",
    "terms in this context. I strongly suggest you further research a little bit yourself into these terms.\n",
    "\n",
    "* *Stationary time-series* (in the ‘weak’ sense). There is no ‘trend’ in the time-series: if you would estimate the mean and variance of a certain range of the time series (say, 10% width), then the two values would not have a trend as you slide this range across the time-series (called a sliding window ). That is, there would be some fluctuation of the two estimated values but an LRM should give slope approximately zero (think back of your hypothesis test in Problem 1). A list of i.i.d. samples is always stationary.\n",
    "* Order of integration. A time-series is first-order integrated if taking the differences $\\vec s_t − \\vec s_{t−1}$ is a stationary process. A stationary process is considered integrated of order zero. A time-series is $x$-order integrated if taking the differences leads to a $(x − 1)$-order integrated time-series. We will not consider $x > 1$ here.\n",
    "* Spurious regression or spurious correlation. Finding a significantly non-zero slope $\\hat \\beta$, or significantly non-zero correlation coefficient (such as r) or goodness-of-fit $r^2$ between two time-series which are actually expected to be uncorrelated. Typically this is due to a common trend, which typically is due to both time-series being first-order integrated (or higher). In turn, this is often because the (non-stationary) time-series are only a small piece of a larger stationary time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Generate an i.i.d. sample $\\vec d$ of $n ≥ 100$ samples using any distribution of your choice for each value to be drawn\n",
    "from. Then generate a second i.i.d. sample $\\vec e$ using any (potentially different) distribution. Calculate\n",
    "the Pearson correlation coefficient r and the percent of variance reduction $r^2$ from these two samples.\n",
    "Next, do a suitable hypothesis test (parametric or non-parametric) to see if $r^2$ = 0 is a plausible value or not.\n",
    "\n",
    "[7 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "\n",
    "Implement your own test of ‘stationarity’ in a function `is_nonstationary(timeseries, p threshold=0.05)`,\n",
    "where the `timeseries` argument is a single list of scalar values. Do this in a suitable way using LRM\n",
    "fitting and a hypothesis test using the `p-value` (so not using the CI as done so far), either parametrically or non-parametrically. It is sufficient to reject the hypothesis of zero slope for the time-series\n",
    "to conclude that the time-series is non-stationary (since there is a trend); you may ignore the variance\n",
    "condition and no use of a sliding window is needed. Let your function return `(True, p value)` if\n",
    "the hypothesis can be rejected, where p value is the numerical p-value of the zero slope; otherwise\n",
    "return (False, p value).<sup>7</sup> Show that the two generated i.i.d. samples are indeed stationary using\n",
    "your function. Also show that `is_nonstationary(range(100), p threshold=0.05)` returns\n",
    "`(True, 0.0)`.\n",
    "\n",
    "<span style=\"font-family:Arial; font-size: 0.8em;\">Note 7: **Warning**: As you well know by now, not being able to reject hypothesis H does not imply that H is true! So you should consider the return value of your function as answering the question: ‘Is the hypothesis of zero slope rejected?’. Not as ‘Is the\n",
    "time-series non-stationary?’. In our case the two statements are the same but more in general we could have more complicated time dependence relationships, like $X(t)=cos(t)$, which may not be well captured supposing a linear relation</span>\n",
    "\n",
    "[9 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nonstationary(timeseries, p_threshold=0.05):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3\n",
    "\n",
    "Use $\\vec d$ and $\\vec e$ to construct first-order integrated time-series $\\vec s_t$ and $\\vec u_t$ . Show that there is a spurious\n",
    "regression (slope of LRM) between the two time-series and that the goodness-of-fit (percent of variance\n",
    "explained) is spurious. Also show that the Pearson correlation coefficient formula returns a spurious\n",
    "correlation value.\n",
    "\n",
    "[5 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4\n",
    "\n",
    "The slope of the LRM between the two time-series which you found in the previous question is usually\n",
    "denoted $β$ in the literature. It is called the *co-integration factor* of the two time-series. If $β \\neq 0$\n",
    "then the two time-series are said to be `co-integrated` and then spurious regression and spurious\n",
    "correlation will take place. (As an aside, the converse is not necessarily true.) So the presence of\n",
    "co-integration acts as a warning sign.\n",
    "\n",
    "Another common definition is as follows. Two time-series are said to be co-integrated if a linear\n",
    "combination of the two time-series has a lower order of integration than the two time-series themselves.\n",
    "Show that such a linear combination exists and that this ‘combined’ time-series indeed has a lower\n",
    "order of integration.\n",
    "\n",
    "[6 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5\n",
    "\n",
    "Describe a general procedure by which the correlation and/or percent of variance reduction (goodness-\n",
    "of-fit) should be computed between two time-series. The time-series may be either stationary or\n",
    "integrated. In your explanation, also briefly mention a mathematical argument in words why the\n",
    "percent of variance reduction (as computed by the LRM procedure using the two original time-series)\n",
    "is only valid when the two time-series are stationary, or better yet, i.i.d\n",
    "\n",
    "[10 bonus points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "**Bonus problem** . On Canvas there is a real dataset uploaded containing the amount of particulate matter\n",
    "(PM) measured by different stations across the Netherlands over time, for a single year computed by averaging\n",
    "over ten years. Different time-series could be extracted from this dataset, such as per weather station, per\n",
    "day, per week, weekly averages over time, per season, etc. Experiment with plotting such different views of\n",
    "the data for yourself.\n",
    "\n",
    "Search for two time-series in this dataset which are co-integrated and thus having a spurious correlation.\n",
    "Estimate this ‘spurious’ correlation value and/or goodness-of-fit and then estimate the ‘true’ correlation\n",
    "value $r$ and/or the goodness-of-fit $r^2$ , using either your LRM procedure (percent of variance reduction) or\n",
    "the Pearson formula. Make your conclusions statistically sound, i.e., use hypothesis tests to conclude with\n",
    "confidence that there is co-integration, and the same for testing stationarity. Show the time-series before and\n",
    "after your transformation(s) and briefly explain whether *a priori* you expected a large or a low correlation.\n",
    "\n",
    "Note: it is also permitted to find a case where the two time-series are integrated but where the correlation is actually not spurious.\n",
    "\n",
    "[15 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
